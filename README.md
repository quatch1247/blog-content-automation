# 목차

1. [과제 목표 및 문제 정의](#과제-목표-및-문제-정의)
2. [접근 방식 및 시스템 구성도](#접근-방식-및-시스템-구성도)
2. [기술적 선택 이유](#기술적-선택-이유)
3. [주요 기능](#주요-기능)
4. [디렉토리 구조](#디렉토리-구조)
5. [실행 방법](#실행-방법)
6. [결과 예시](#결과-예시)
7. [개선 아이디어](#개선-아이디어)


## 과제 목표 및 문제 정의

프로젝트의 목표는 **각각의 PDF에 포함된 여러 블로그 포스트를 자동으로 분리·정제하고, 요약 및 메타데이터를 구조화하여 적재한 뒤 이를 활용해 콘텐츠 생성 과정을 자동화하는 시스템**을 구축하는 것입니다.

기존에는 하나의 PDF 안에 여러 포스트가 섞여 있어  
- 수작업으로 텍스트를 추출해야 하고,  
- 요약, 재작성, 썸네일 제작 등의 과정을 개별적으로 수행해야 하는 비효율이 있었습니다.



## 접근 방식 및 시스템 구성도

### 접근 방식

이를 해결하기 위해,  
> **“PDF 업로드 → 포스트 단위 분리 → 텍스트 정제 → 요약 → 데이터 적재”**  
까지의 과정을 자동화한 **로컬 기반 ETL 파이프라인**을 구현했습니다.

파이프라인은 FastAPI 환경에서 실행되며  
- **PDF 업로드 시 자동으로 포스트 단위 분리 및 정제**,  
- **추후 콘텐츠 생성을 위한 요약(마크다운/간략 요약)**,  
- **결과 데이터를 PostgreSQL(DB 컨테이너) 및 로컬 스토리지(`uploads/`)에 적재**하도록 구성되어 있습니다.

---

### 데이터 적재 구조

모든 처리 결과는 **PostgreSQL 컨테이너**에 메타데이터로 저장되고,  
본문·이미지·요약 파일은 **로컬 컨테이너 내 `uploads/` 디렉토리**에 보존됩니다.

- **RDB**: 구조화된 데이터의 Persistent Storage 담당  
- **로컬 스토리지**: 실제 파일 데이터의 Persistent Storage 담당  
- **Docker 볼륨**: 위 두 영역 모두를 유지

| 테이블 | 역할 |
|--------|------|
| **RawPdf** | 업로드된 원본 PDF 정보 |
| **SplitPost** | PDF에서 분리된 개별 포스트 단위 |
| **RefinedPost** | 정제 및 요약된 포스트 데이터 |

<p>
  <img src="./images/스크린샷 2025-10-14 오후 9.39.40.png" alt="ERD 스크린샷" width="600"/>
</p>

데이터베이스에는 위 3단계 구조로 각 처리 결과가 기록되며,  
로컬 저장소(`uploads/raw_pdf`, `uploads/split_posts`, `uploads/refined_posts`)에는 실제 파일이 계층적으로 저장됩니다.

특히 `uploads/refined_posts` 디렉터리에는 각 블로그 포스트가  
아래와 같이 **JSON 형식으로 구조화**되어 저장됩니다.  
이 파일에는 제목(`title`), 작성자(`author`), 작성일(`date`), 원문 URL(`url`), 본문 내용(`body`) 등이 포함되며,  
추가적으로 추출된 이미지 파일들도 함께 저장됩니다.

<p>
  <img src="./images/스크린샷 2025-10-14 오후 9.48.10.png" alt="포스팅 파싱 스크린샷" width="600"/>
</p>


### ETL 파이프라인 개요

#### 데이터 수집
- 업로드된 PDF를 `pdfminer`로 HTML로 변환  
- `<a name="n">Page n</a>` 패턴을 기준으로 페이지 맵(`page_map`) 구성  
- 정규표현식을 활용해 포스트 시작 페이지를 탐지하고,  
  각 포스트의 페이지 범위(`post_ranges`)를 자동 산출

#### 데이터 변환
- 감지된 범위별로 PDF를 분리한 뒤 `BeautifulSoup`으로 텍스트 추출 및 병합  
- 작성자, 날짜, URL 등의 메타데이터를 파싱  
- `fitz(PyMuPDF)`로 링크와 겹치지 않는 이미지만 추출하여 저장  
- 반복 문구나 지저분한 텍스트를 제거하여 정제 단계 수행함 
- 결과를 **포스트 단위 JSON** 형태로 저장

#### 데이터 적재 및 응용
- 정제된 JSON 및 이미지 경로를 DB와 파일 시스템에 적재  
- 이후 **OpenAI API**를 이용하여 다음을 수행:
  - 마크다운 형식을 변환한 pdf 요약문 자동 생성  
  - 제목·본문 기반 썸네일 이미지 생성  
  - 지정 기간(`start_date ~ end_date`)별 인사이트 리포트(pdf) 생성  


## 기술적 선택 이유

#### FastAPI
- **확장성**: API 기반 구조로, 웹 대시보드·자동 배치·외부 연동 등으로 확장 용이
- **비동기 I/O**: PDF 파싱·AI 요청을 병렬화하여 응답 속도 향상  
- **유지보수성**: `Service / Repository / Schema` 계층으로 모듈화된 구조
- **SwaggerUI**: 리소스상 UI 구현 대신, 시스템 핵심 로직 개발에 집중하기 위해 API 테스트 및 인터페이스 역할을 동시에 수행할 수 있도록 설계

#### OpenAI API
- **품질 및 안정성**: Groq, Stable Diffusion 등 오픈 api 및 모델을 고려 했으나, 테스트 결과 품질 및 속도가 느려서 비활성화
- **로컬 모델 한계 보완**: 오픈 소스 생성형 모델을 서빙하기에 리소스가 부족

#### PostgreSQL
- 데이터간 관계정의와 정합성 검증(트랜잭션 처리)를 위해 사용

#### Docker
- 환경(os등) 일관성 확보 및 손쉬운 유지보수를 위해 사용

---

### 사전 요약(Markdown) 저장 설계 배경

OpenAI API로 생성된 요약문은  
Markdown 형태로 db 저장되며,  
핵심 키워드 기반 요약 컬럼을 함께 DB에 미리 적재합니다.

이 설계에는 명확한 트레이드오프가 존재합니다.

| 항목 | 선택 이유 | 단점 |
|------|------------|------|
| **사전 생성(Precompute)** | 업로드 시 한 번에 요약 및 저장 → 이후 조회 시 즉시 응답 | 초기 업로드 시간이 다소 길어짐 |
| **요청 시 생성(On-demand)** | 저장 공간 절약, 최신 모델 반영 가능 | 호출 시마다 OpenAI 응답 대기(지연) 발생 |

따라서 본 프로젝트에서 **더 빠른 응답형 콘텐츠 생성 흐름**(사용자에게 요약·리라이팅·리포트·이미지 자동화)을 지원하기 위해  
**사전 생성 방식**을 채택했습니다.
<p>
  <img src="./images/스크린샷 2025-10-15 오전 8.26.15.png" alt="요약 데이터 사전 생성" width="600"/>
</p>

<p>
  본문으로부터 핵심 문장과 주요 키워드 추출 결과
</p>


## 주요 기능

### Files
- `POST /files/upload/pdf` — 단일 PDF 업로드 및 자동 분리
- `POST /files/upload/pdf/multi` — 다중 PDF 업로드 처리
- `GET /files/` — 업로드된 파일 및 정제 결과 목록 조회(id = post_id 조회)
- `DELETE /files/truncate/all` — 업로드 및 처리된 모든 데이터 초기화


### Thumbnails
- `GET /thumbnails/download/{post_id}` — 포스트별 자동 생성 썸네일 이미지 다운로드


### Summaries
- `GET /summaries/download/{post_id}` — 포스트 요약문을 PDF로 다운로드


### Period Report
- `GET /period-report/` — 지정된 기간(start_date~end_date)의 인사이트 리포트 PDF 다운로드



## 디렉토리 구조

<p>
  <img src="./images/스크린샷 2025-10-14 오후 10.05.17.png" alt="디렉토리 스크린샷" width="600"/>
</p>


## 실행 방법

### 1. 프로젝트 클론

```bash
git clone https://github.com/your-username/blog-content-automation.git
cd blog-content-automation
```

### 2. 환경 변수 설정 (.env)
> ❗프로젝트 루트에 `.env` 파일을 반드시 생성해야 합니다.

```bash
# 예시
DB_USER=
DB_PASSWORD=
DB_DATABASE=
DB_HOST=
DB_PORT=

DB_POOL_SIZE=
DB_MAX_OVERFLOW=
DB_POOL_TIMEOUT=
DB_POOL_RECYCLE=

ENVIRONMENT=
PROJECT_NAME=
CORS_ORIGINS_RAW=

OPENAI_API_KEY=
```

### 3. Docker Compose 실행
아래 명령어로 FastAPI, PostgreSQL, pgAdmin 서비스를 한 번에 실행합니다.

```bash
docker compose up -d --build
```


### 4. 서비스 접속 경로
| 서비스 | 설명 | 주소 |
|:--------|:-------------------------------|:-----------------------------|
| **FastAPI** | 블로그 자동화 API 서버 | [http://localhost:8000/docs](http://localhost:8000/docs) |
| **pgAdmin4** | 데이터베이스 관리 대시보드 | [http://localhost:5050](http://localhost:5050) |

---

### pgAdmin 기본 계정

| 항목 | 값 |
|:------|:----------------|
| **ID** | `admin@local.com` |
| **PW** | `admin1234` |

**서버 등록**  
1. 좌측 상단 메뉴에서 **"Servers → Register → Server"** 선택  
2. `General` 탭 → 이름(Name): `Local Postgres` 입력  
3. `Connection` 탭에서 DB 정보 입력 

> 💡 pgAdmin에서 PostgreSQL 접속 시 필요한 **Host**, **Port**, **Database**, **User**, **Password** 정보는  
> 프로젝트 루트의 `.env` 파일을 참고하세요. 




## 결과 예시

### 1. PDF 업로드 및 리스트 확인
<p>
  <img src="./images/스크린샷 2025-10-14 오후 11.01.54.png" alt="업로드 스크린샷 1" height="300">
  <img src="./images/스크린샷 2025-10-14 오후 11.03.50.png" alt="업로드 스크린샷 2" height="300">
</p>

### 2. 포스팅 정제 결과
<p>
  <img src="./images/스크린샷 2025-10-14 오후 11.08.43.png" alt="DB 스크린샷 1" height="300">
</p>
> 분리된 각 포스트를 자동으로 텍스트 정제 및 요약

### 3. 요약 / 리라이팅
<p>
  <img src="./images/스크린샷 2025-10-14 오후 11.19.59.png" alt="요약 리라이팅 스크린샷 1" height="300">
</p>
> 기존 포스트를 짧게 요약하거나 최신 문체로 재작성

### 4. 썸네일 이미지 자동 생성
<p>
  <img src="./images/thumbnail_2025_10_14_23_13.png" alt="썸네일 스크린샷 1" height="300">
</p>
> 제목 및 본문을 기반으로 AI를 활용해 자동으로 생성된 썸네일입니다.


### 5. 기간별 리포트 PDF
<p>
  <img src="./images/스크린샷 2025-10-14 오후 11.28.10.png" alt="콘텐츠 기간별 스크린샷 1" height="300">
</p>
> 지정한 기간 내의 포스트 요약을 기반으로 자동 생성된 콘텐츠 큐레이션 리포트 입니다.


## 개선 아이디어

### 1. 다중 파일 업로드 시 비동기 백그라운드 처리

현재의 시스템은 다수의 PDF 파일을 동시에 업로드하는 경우에는 처리량이 급격히 증가합니다.  
이를 해결하기 위해 다음과 같은 **비동기 백그라운드 태스크 큐 구조**를 적용할 수 있습니다.

- **기술 스택 제안:** `Celery + Redis`  
- **개선 포인트:**
  - 여러 PDF를 동시에 업로드하더라도 **비동기 분산 처리** 가능  
  - 각 태스크별 **상태 추적** 및 **실패 시 재시도** 지원  
  - ETL 단계(추출 → 분리 → 요약 → 리포트)별 **로그 및 결과 관리 자동화**

---

### 2. 모델 학습 및 응답 일관성 향상

현재 시스템은 OpenAI API의 모델을 활용하지만,  
**도메인 특화 요약**이나 **응답 일관성 유지**를 위해 다음 확장 방안을 고려할 수 있습니다.

- **로컬 모델 전환**  
  - OpenAI API에 전적으로 의존하지 않도록,  
    추후 **LLaMA / Mistral / Gemma** 등 로컬 경량 모델을 활용 및 평가 
  - 사내 데이터 보안·비용 절감 측면에서도 유리

- **RAG(Retrieval-Augmented Generation) 기반 문맥 주입**  
  - 모든 PDF의 메타데이터·요약문을 벡터화하여 Pinecone 등 벡터DB에 저장  
  - 요약 또는 리포트 생성 시 관련 문맥을 검색해 **외부지식을 실시간으로 주입**  
  - 문맥 주입시에 무엇을 기준으로 관련 문맥을 가져올지 요구사항 파
  - 파인튜닝 없이도 **응답 일관성 향상 + 최신 정보 반영** 가능

- **파인튜닝(Fine-tuning)**  
  - 기존 PDF 데이터셋(요약문, 메타데이터 등)을 학습시켜  
    모델이 “블로그 콘텐츠 문체와 구조”에 특화되도록 미세 조정  
  - 단, **비용·데이터 관리·성능 검증의 복잡성**이 매우 높음  
    결론 : MVP 단계에서는 RAG 방식으로 확장성을 확보하고, 추후 서비스가 안정화되면 파인튜닝 및 로컬 모델로의 전환을 검토하는 것이 이상적.

---
## 3. 사용자 검수 및 이미지 활용 고도화

### 3-1. LLM 초안의 한계와 사용자 검수 프로세스 필요성

LLM(대형언어모델)은 초안 생성에는 매우 유용하지만, **공적인 문서**나 **대외 게시용 콘텐츠**에는 오류나 할루시네이션이 발생할 수 있습니다.  
따라서 서비스 구조상 **AI 초안 → 사용자 검수 → 확정본 저장**의 단계를 거치는 것이 바람직합니다.

- **초안 단계:** LLM이 1차 콘텐츠 생성
- **검수 단계:** 사람이 문맥, 사실관계, 표현을 점검
- **확정 단계:** 교정된 버전을 DB 또는 게시용 파일로 저장

만약 서비스에 적용한다면, AI 작성 보조 도구로서의 서비스 신뢰성을 높이는 것이 필요해보입니다.
-> 사용자 책임제

---

### 3-2. 이미지 활용의 방향성과 한계

현재 프로젝트에서 PDF에서 이미지 파일을 수집하고 있지만, 이를 본문 문맥에 직접 활용하지 않는 이유는 다음과 같습니다.

- 이미지 내 텍스트(OCR 결과)의 **정확성과 일관성이 부족**
- 이미지가 본문에 게시자가 삽입한게 아닌, 특정 링크의 썸네일인 경우
- 단순 OCR 텍스트를 본문에 삽입할 경우 **맥락 왜곡 및 품질 저하** 위험
- 본문과의 **의미적 연관성**을 판별하는 로직 부재 → 오버엔지니어링 가능성

다만, 이미지가 본문의 흐름을 보조하거나 **시각적 맥락을 강화**할 수 있는 경우,  
**이미지-텍스트 연관성 판단 모델**을 통해  
의미적 유사도를 측정하고,  
**자동 캡션 생성** 또는 **연관 이미지 추천 기능**으로 확장할 수 있습니다.

---

### 3-3. PDF 기반 수집의 한계와 HTML 크롤링의 대안

현재는 **PDF를 파싱하여 텍스트를 추출**하고 있으나,  
사실상 **원본 블로그를 직접 크롤링**하는 것이  
정보 풍부도·정밀도 측면에서 훨씬 유리해 보입니다.

현재 시스템은 PDF 내 다수의 포스팅을 **자동 분리**한 뒤,  
각 포스팅으로부터 **원문 블로그 주소를 파싱**하는 단계까지 구현되어 있습니다.  
즉, PDF를 통해서도 어느 정도 구조적 콘텐츠 추출이 가능하지만,  
이는 결국 **원문 웹페이지의 풍부한 정보(HTML 태그, 메타데이터, 링크 구조 등)**를  
간접적으로 복원하는 과정에 불과합니다.

이러한 점에서, **HTML 크롤링 기반의 직접 수집 방식**으로 전환하면  
PDF 파싱보다 훨씬 풍부한 텍스트·태그·메타데이터를 확보할 수 있습니다.

---